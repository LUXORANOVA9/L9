# LUXOR AI CUA - å¤‡ä»½æ¢å¤ä¸æ¨¡å‹é›†æˆæŒ‡å—
## Backup Restoration & Model Integration Guide

### ğŸ“¦ å¤‡ä»½æ–‡ä»¶åˆ†æ / Backup Files Analysis

åŸºäºæ‚¨æä¾›çš„å¤‡ä»½æ–‡ä»¶åˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦æ¢å¤ä»¥ä¸‹ç»„ä»¶ï¼š

```
E:\luxor9\backup_20250722_003534\
â”œâ”€â”€ luxor9_ollama_models.tar.gz          # Ollama AIæ¨¡å‹å¤‡ä»½
â”œâ”€â”€ anaconda_ollama_models.tar.gz        # Anacondaç¯å¢ƒä¸­çš„Ollamaæ¨¡å‹
â”œâ”€â”€ containers.tar.gz                    # Dockerå®¹å™¨å¤‡ä»½
â”œâ”€â”€ images.tar.gz                        # Dockeré•œåƒå¤‡ä»½
â””â”€â”€ luxor9_luxor9_ollama_models.tar.gz   # é‡å¤çš„Ollamaæ¨¡å‹å¤‡ä»½
```

### ğŸ”„ æ¢å¤ç­–ç•¥ / Restoration Strategy

#### 1. åˆ›å»ºæ¨¡å‹å­˜å‚¨ç›®å½•ç»“æ„

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºAIæ¨¡å‹å­˜å‚¨ç»“æ„
mkdir -p ai-models/{ollama,huggingface,custom}
mkdir -p ai-models/ollama/{chinese-models,code-models,embeddings}
mkdir -p docker-backup/{containers,images}
mkdir -p environments/{anaconda,python}
```

#### 2. æ¨¡å‹æ¢å¤ä¼˜å…ˆçº§

**é«˜ä¼˜å…ˆçº§ (ç«‹å³æ¢å¤)**:
- `luxor9_ollama_models.tar.gz` - æ ¸å¿ƒOllamaæ¨¡å‹
- `images.tar.gz` - Dockeré•œåƒ (åŒ…å«é¢„é…ç½®çš„AIæœåŠ¡)

**ä¸­ä¼˜å…ˆçº§ (æŒ‰éœ€æ¢å¤)**:
- `anaconda_ollama_models.tar.gz` - Anacondaç¯å¢ƒæ¨¡å‹
- `containers.tar.gz` - Dockerå®¹å™¨çŠ¶æ€

**ä½ä¼˜å…ˆçº§ (éªŒè¯åæ¢å¤)**:
- `luxor9_luxor9_ollama_models.tar.gz` - å¯èƒ½æ˜¯é‡å¤å¤‡ä»½

### ğŸš€ è‡ªåŠ¨åŒ–æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# restore-ai-models.sh - LUXOR AI CUA æ¨¡å‹æ¢å¤è„šæœ¬

set -e

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

info() { echo -e "${BLUE}â„¹ï¸  $1${NC}"; }
success() { echo -e "${GREEN}âœ… $1${NC}"; }
warn() { echo -e "${YELLOW}âš ï¸  $1${NC}"; }
error() { echo -e "${RED}âŒ $1${NC}"; }

# é…ç½®å˜é‡
BACKUP_DIR="E:/luxor9/backup_20250722_003534"
PROJECT_ROOT="E:/Project_Luxor9"
MODELS_DIR="$PROJECT_ROOT/ai-models"
DOCKER_BACKUP_DIR="$PROJECT_ROOT/docker-backup"

# æ£€æŸ¥å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
check_backup_files() {
    info "æ£€æŸ¥å¤‡ä»½æ–‡ä»¶..."
    
    local backup_files=(
        "luxor9_ollama_models.tar.gz"
        "anaconda_ollama_models.tar.gz"
        "containers.tar.gz"
        "images.tar.gz"
        "luxor9_luxor9_ollama_models.tar.gz"
    )
    
    for file in "${backup_files[@]}"; do
        if [ -f "$BACKUP_DIR/$file" ]; then
            success "æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶: $file"
        else
            warn "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $file"
        fi
    done
}

# åˆ›å»ºç›®å½•ç»“æ„
create_directories() {
    info "åˆ›å»ºç›®å½•ç»“æ„..."
    
    mkdir -p "$MODELS_DIR"/{ollama,huggingface,custom}
    mkdir -p "$MODELS_DIR"/ollama/{chinese-models,code-models,embeddings}
    mkdir -p "$DOCKER_BACKUP_DIR"/{containers,images}
    mkdir -p "$PROJECT_ROOT"/environments/{anaconda,python}
    
    success "ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
}

# æ¢å¤Ollamaæ¨¡å‹
restore_ollama_models() {
    info "æ¢å¤Ollamaæ¨¡å‹..."
    
    if [ -f "$BACKUP_DIR/luxor9_ollama_models.tar.gz" ]; then
        info "è§£å‹ä¸»è¦Ollamaæ¨¡å‹å¤‡ä»½..."
        tar -xzf "$BACKUP_DIR/luxor9_ollama_models.tar.gz" -C "$MODELS_DIR/ollama/"
        success "Ollamaæ¨¡å‹æ¢å¤å®Œæˆ"
        
        # åˆ—å‡ºæ¢å¤çš„æ¨¡å‹
        info "å·²æ¢å¤çš„æ¨¡å‹åˆ—è¡¨:"
        find "$MODELS_DIR/ollama/" -name "*.bin" -o -name "*.gguf" -o -name "*.safetensors" | head -10
    else
        warn "Ollamaæ¨¡å‹å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
    fi
}

# æ¢å¤Dockeré•œåƒ
restore_docker_images() {
    info "æ¢å¤Dockeré•œåƒ..."
    
    if [ -f "$BACKUP_DIR/images.tar.gz" ]; then
        info "è§£å‹Dockeré•œåƒå¤‡ä»½..."
        tar -xzf "$BACKUP_DIR/images.tar.gz" -C "$DOCKER_BACKUP_DIR/images/"
        
        # åŠ è½½Dockeré•œåƒ (å¦‚æœDockeræ­£åœ¨è¿è¡Œ)
        if command -v docker &> /dev/null && docker info &> /dev/null; then
            info "åŠ è½½Dockeré•œåƒåˆ°æœ¬åœ°Docker daemon..."
            find "$DOCKER_BACKUP_DIR/images/" -name "*.tar" -exec docker load -i {} \;
            success "Dockeré•œåƒåŠ è½½å®Œæˆ"
        else
            warn "Dockeræœªè¿è¡Œï¼Œé•œåƒæ–‡ä»¶å·²è§£å‹åˆ° $DOCKER_BACKUP_DIR/images/"
        fi
    else
        warn "Dockeré•œåƒå¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
    fi
}

# æ¢å¤Anacondaç¯å¢ƒ
restore_anaconda_models() {
    info "æ¢å¤Anacondaç¯å¢ƒæ¨¡å‹..."
    
    if [ -f "$BACKUP_DIR/anaconda_ollama_models.tar.gz" ]; then
        info "è§£å‹Anaconda Ollamaæ¨¡å‹..."
        tar -xzf "$BACKUP_DIR/anaconda_ollama_models.tar.gz" -C "$PROJECT_ROOT/environments/anaconda/"
        success "Anacondaæ¨¡å‹æ¢å¤å®Œæˆ"
    else
        warn "Anacondaæ¨¡å‹å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
    fi
}

# æ¢å¤Dockerå®¹å™¨
restore_docker_containers() {
    info "æ¢å¤Dockerå®¹å™¨é…ç½®..."
    
    if [ -f "$BACKUP_DIR/containers.tar.gz" ]; then
        info "è§£å‹Dockerå®¹å™¨å¤‡ä»½..."
        tar -xzf "$BACKUP_DIR/containers.tar.gz" -C "$DOCKER_BACKUP_DIR/containers/"
        success "Dockerå®¹å™¨é…ç½®æ¢å¤å®Œæˆ"
        
        info "å®¹å™¨é…ç½®æ–‡ä»¶ä½äº: $DOCKER_BACKUP_DIR/containers/"
        info "å¯ä»¥ä½¿ç”¨è¿™äº›é…ç½®é‡æ–°åˆ›å»ºå®¹å™¨"
    else
        warn "Dockerå®¹å™¨å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
    fi
}

# éªŒè¯æ¨¡å‹å®Œæ•´æ€§
verify_models() {
    info "éªŒè¯æ¢å¤çš„æ¨¡å‹..."
    
    local model_count=0
    local total_size=0
    
    if [ -d "$MODELS_DIR/ollama" ]; then
        model_count=$(find "$MODELS_DIR/ollama" -type f \( -name "*.bin" -o -name "*.gguf" -o -name "*.safetensors" \) | wc -l)
        total_size=$(du -sh "$MODELS_DIR/ollama" | cut -f1)
        
        success "å‘ç° $model_count ä¸ªæ¨¡å‹æ–‡ä»¶"
        success "æ€»å¤§å°: $total_size"
    fi
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æ¨¡å‹
    if find "$MODELS_DIR" -name "*chatglm*" -o -name "*baichuan*" -o -name "*qwen*" | grep -q .; then
        success "æ£€æµ‹åˆ°ä¸­æ–‡è¯­è¨€æ¨¡å‹"
    else
        warn "æœªæ£€æµ‹åˆ°ä¸­æ–‡è¯­è¨€æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦é¢å¤–ä¸‹è½½"
    fi
}

# ç”Ÿæˆæ¨¡å‹é…ç½®æ–‡ä»¶
generate_model_config() {
    info "ç”Ÿæˆæ¨¡å‹é…ç½®æ–‡ä»¶..."
    
    cat > "$MODELS_DIR/model-config.yml" << EOF
# LUXOR AI CUA æ¨¡å‹é…ç½®
models:
  chinese_nlp:
    # ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹
    chatglm3:
      path: "./ollama/chinese-models/chatglm3-6b"
      type: "conversational"
      language: "zh-CN"
    
    qwen:
      path: "./ollama/chinese-models/qwen-14b"
      type: "conversational" 
      language: "zh-CN"
      
    baichuan2:
      path: "./ollama/chinese-models/baichuan2-13b"
      type: "conversational"
      language: "zh-CN"
  
  code_generation:
    # ä»£ç ç”Ÿæˆæ¨¡å‹
    codellama:
      path: "./ollama/code-models/codellama-7b"
      type: "code"
      languages: ["python", "javascript", "java", "go"]
    
    starcoder:
      path: "./ollama/code-models/starcoder-15b"
      type: "code"
      languages: ["python", "javascript", "typescript", "java"]
  
  embeddings:
    # å‘é‡åµŒå…¥æ¨¡å‹
    bge_large:
      path: "./ollama/embeddings/bge-large-zh"
      type: "embedding"
      language: "zh-CN"
      dimensions: 1024

# OllamaæœåŠ¡é…ç½®
ollama:
  host: "localhost"
  port: 11434
  models_path: "./ollama"
  
# Dockeré…ç½®
docker:
  images_path: "../docker-backup/images"
  containers_path: "../docker-backup/containers"
EOF

    success "æ¨¡å‹é…ç½®æ–‡ä»¶å·²ç”Ÿæˆ: $MODELS_DIR/model-config.yml"
}

# æ›´æ–°é¡¹ç›®é…ç½®
update_project_config() {
    info "æ›´æ–°é¡¹ç›®é…ç½®æ–‡ä»¶..."
    
    # æ›´æ–° .env æ–‡ä»¶ä»¥åŒ…å«æ¨¡å‹è·¯å¾„
    if [ -f "$PROJECT_ROOT/.env" ]; then
        # æ·»åŠ æˆ–æ›´æ–°æ¨¡å‹ç›¸å…³é…ç½®
        cat >> "$PROJECT_ROOT/.env" << EOF

# AIæ¨¡å‹é…ç½® (ä»å¤‡ä»½æ¢å¤)
MODELS_BASE_PATH=$MODELS_DIR
OLLAMA_MODELS_PATH=$MODELS_DIR/ollama
CHINESE_MODEL_PATH=$MODELS_DIR/ollama/chinese-models/chatglm3-6b
CODE_MODEL_PATH=$MODELS_DIR/ollama/code-models/codellama-7b
EMBEDDING_MODEL_PATH=$MODELS_DIR/ollama/embeddings/bge-large-zh

# OllamaæœåŠ¡é…ç½®
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_KEEP_ALIVE=24h

# æ¨¡å‹é…ç½®æ–‡ä»¶
MODEL_CONFIG_PATH=$MODELS_DIR/model-config.yml
EOF
        success "ç¯å¢ƒé…ç½®å·²æ›´æ–°"
    fi
}

# åˆ›å»ºæ¨¡å‹ç®¡ç†è„šæœ¬
create_model_manager() {
    info "åˆ›å»ºæ¨¡å‹ç®¡ç†è„šæœ¬..."
    
    cat > "$PROJECT_ROOT/scripts/manage-models.sh" << 'EOF'
#!/bin/bash
# LUXOR AI CUA æ¨¡å‹ç®¡ç†è„šæœ¬

MODELS_DIR="./ai-models"
OLLAMA_HOST="localhost:11434"

list_models() {
    echo "=== å·²å®‰è£…çš„æ¨¡å‹ ==="
    if command -v ollama &> /dev/null; then
        ollama list
    else
        echo "Ollamaæœªå®‰è£…æˆ–æœªè¿è¡Œ"
        echo "æœ¬åœ°æ¨¡å‹æ–‡ä»¶:"
        find "$MODELS_DIR" -name "*.gguf" -o -name "*.bin" -o -name "*.safetensors" | head -10
    fi
}

start_ollama() {
    echo "=== å¯åŠ¨OllamaæœåŠ¡ ==="
    if command -v ollama &> /dev/null; then
        ollama serve &
        echo "OllamaæœåŠ¡å·²åœ¨åå°å¯åŠ¨"
    else
        echo "Ollamaæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Ollama"
    fi
}

load_model() {
    local model_name=$1
    if [ -z "$model_name" ]; then
        echo "ä½¿ç”¨æ–¹æ³•: $0 load <model_name>"
        return 1
    fi
    
    echo "=== åŠ è½½æ¨¡å‹: $model_name ==="
    ollama pull "$model_name"
}

test_model() {
    local model_name=${1:-"chatglm3"}
    echo "=== æµ‹è¯•æ¨¡å‹: $model_name ==="
    echo "ä½ å¥½ï¼Œæˆ‘æ˜¯LUXOR AIåŠ©æ‰‹" | ollama run "$model_name"
}

case "$1" in
    "list"|"ls")
        list_models
        ;;
    "start")
        start_ollama
        ;;
    "load")
        load_model "$2"
        ;;
    "test")
        test_model "$2"
        ;;
    *)
        echo "LUXOR AI CUA æ¨¡å‹ç®¡ç†å™¨"
        echo "ä½¿ç”¨æ–¹æ³•: $0 {list|start|load|test} [å‚æ•°]"
        echo ""
        echo "å‘½ä»¤:"
        echo "  list          åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"
        echo "  start         å¯åŠ¨OllamaæœåŠ¡"
        echo "  load <name>   åŠ è½½æŒ‡å®šæ¨¡å‹"
        echo "  test [name]   æµ‹è¯•æ¨¡å‹åŠŸèƒ½"
        ;;
esac
EOF

    chmod +x "$PROJECT_ROOT/scripts/manage-models.sh"
    success "æ¨¡å‹ç®¡ç†è„šæœ¬å·²åˆ›å»º"
}

# ä¸»å‡½æ•°
main() {
    echo "ğŸ¤– LUXOR AI CUA - æ¨¡å‹æ¢å¤åŠ©æ‰‹"
    echo "================================"
    
    check_backup_files
    create_directories
    restore_ollama_models
    restore_docker_images
    restore_anaconda_models
    restore_docker_containers
    verify_models
    generate_model_config
    update_project_config
    create_model_manager
    
    echo ""
    success "ğŸ‰ æ¨¡å‹æ¢å¤å®Œæˆ!"
    echo ""
    info "ä¸‹ä¸€æ­¥æ“ä½œ:"
    echo "1. éªŒè¯æ¨¡å‹: ./scripts/manage-models.sh list"
    echo "2. å¯åŠ¨Ollama: ./scripts/manage-models.sh start"
    echo "3. æµ‹è¯•æ¨¡å‹: ./scripts/manage-models.sh test"
    echo "4. è¿è¡ŒLUXOR AI: ./quick-start.sh"
}

# æ‰§è¡Œä¸»å‡½æ•°
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
```

### ğŸ”§ Dockeré›†æˆç­–ç•¥

#### 1. æ¢å¤çš„é•œåƒé›†æˆåˆ°é¡¹ç›®

```yaml
# docker-compose.ai-models.yml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./ai-models/ollama:/root/.ollama
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  model-manager:
    build:
      context: ./backend
      dockerfile: Dockerfile.model-manager
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - MODELS_CONFIG_PATH=/app/ai-models/model-config.yml
    volumes:
      - ./ai-models:/app/ai-models
    ports:
      - "8001:8001"

volumes:
  ollama_data:
```

#### 2. æ¨¡å‹æœåŠ¡API

```python
# backend/services/model_service.py
import ollama
import yaml
from pathlib import Path
from typing import List, Dict, Optional

class ModelService:
    """LUXOR AI CUA æ¨¡å‹æœåŠ¡"""
    
    def __init__(self, config_path: str = "./ai-models/model-config.yml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.ollama_client = ollama.Client(
            host=self.config.get('ollama', {}).get('host', 'localhost:11434')
        )
    
    def _load_config(self) -> Dict:
        """åŠ è½½æ¨¡å‹é…ç½®"""
        if self.config_path.exists():
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}
    
    def list_available_models(self) -> List[Dict]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        try:
            models = self.ollama_client.list()
            return models.get('models', [])
        except Exception as e:
            print(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def get_chinese_model(self) -> Optional[str]:
        """è·å–æœ€ä½³ä¸­æ–‡æ¨¡å‹"""
        chinese_models = self.config.get('models', {}).get('chinese_nlp', {})
        
        # æŒ‰ä¼˜å…ˆçº§æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        priority_models = ['chatglm3', 'qwen', 'baichuan2']
        
        for model_key in priority_models:
            if model_key in chinese_models:
                model_path = chinese_models[model_key]['path']
                if self._is_model_available(model_key):
                    return model_key
        
        return None
    
    def get_code_model(self) -> Optional[str]:
        """è·å–æœ€ä½³ä»£ç ç”Ÿæˆæ¨¡å‹"""
        code_models = self.config.get('models', {}).get('code_generation', {})
        
        priority_models = ['codellama', 'starcoder']
        
        for model_key in priority_models:
            if model_key in code_models:
                if self._is_model_available(model_key):
                    return model_key
        
        return None
    
    def _is_model_available(self, model_name: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
        try:
            models = self.list_available_models()
            return any(model['name'].startswith(model_name) for model in models)
        except:
            return False
    
    async def generate_text(self, prompt: str, model: str = None) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not model:
            model = self.get_chinese_model()
        
        if not model:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
        
        try:
            response = self.ollama_client.generate(
                model=model,
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'max_tokens': 2048
                }
            )
            return response.get('response', '')
        except Exception as e:
            raise Exception(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
    
    async def generate_code(self, description: str, language: str = 'python') -> str:
        """ç”Ÿæˆä»£ç """
        model = self.get_code_model()
        if not model:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„ä»£ç ç”Ÿæˆæ¨¡å‹")
        
        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹æè¿°ç”Ÿæˆ{language}ä»£ç :
{description}

è¦æ±‚:
1. ä»£ç ç»“æ„æ¸…æ™°
2. åŒ…å«å¿…è¦çš„æ³¨é‡Š
3. éµå¾ªæœ€ä½³å®è·µ
4. åŒ…å«é”™è¯¯å¤„ç†

ä»£ç :
```{language}
"""
        
        return await self.generate_text(prompt, model)
```

### ğŸ“± ä½¿ç”¨è¯´æ˜

#### 1. ç«‹å³æ‰§è¡Œæ¢å¤

```bash
# 1. ä¿å­˜æ¢å¤è„šæœ¬
chmod +x restore-ai-models.sh

# 2. æ‰§è¡Œæ¢å¤
./restore-ai-models.sh

# 3. éªŒè¯æ¢å¤ç»“æœ
ls -la ai-models/ollama/
```

#### 2. é›†æˆåˆ°ç°æœ‰é¡¹ç›®

```bash
# 1. æ›´æ–°é¡¹ç›®é…ç½®
source .env

# 2. å¯åŠ¨AIæ¨¡å‹æœåŠ¡
docker-compose -f docker-compose.yml -f docker-compose.ai-models.yml up -d

# 3. æµ‹è¯•æ¨¡å‹åŠŸèƒ½
./scripts/manage-models.sh test
```

### ğŸ¯ é›†æˆä¼˜åŠ¿

1. **å®Œæ•´çš„AIèƒ½åŠ›**: æ¢å¤æ‰€æœ‰å¤‡ä»½çš„æ¨¡å‹ï¼Œæä¾›å®Œæ•´çš„AIåŠŸèƒ½
2. **ä¸­æ–‡ä¼˜åŒ–**: ä¸“é—¨é’ˆå¯¹ä¸­æ–‡åœºæ™¯ä¼˜åŒ–çš„æ¨¡å‹é…ç½®
3. **ä»£ç ç”Ÿæˆ**: æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€çš„ä»£ç ç”Ÿæˆ
4. **æœ¬åœ°éƒ¨ç½²**: æ‰€æœ‰æ¨¡å‹æœ¬åœ°è¿è¡Œï¼Œä¿è¯æ•°æ®å®‰å…¨
5. **å®¹å™¨åŒ–ç®¡ç†**: é€šè¿‡Dockerç»Ÿä¸€ç®¡ç†æ‰€æœ‰AIæœåŠ¡

è¿™ä¸ªæ¢å¤æ–¹æ¡ˆå°†æ‚¨çš„å¤‡ä»½æ¨¡å‹å®Œç¾é›†æˆåˆ°LUXOR AI CUAé¡¹ç›®ä¸­ï¼Œè®©ç³»ç»Ÿå…·å¤‡çœŸæ­£çš„AIä»£ç ç”Ÿæˆå’Œä¸­æ–‡ç†è§£èƒ½åŠ›ï¼