#!/bin/bash
# restore-ai-models.sh - LUXOR AI CUA æ¨¡å‹æ¢å¤è„šæœ¬

set -e

# é…ç½®å˜é‡
BACKUP_DIR="E:/luxor9/backup_20250722_003534"
PROJECT_ROOT="E:/Project_Luxor9"
MODELS_DIR="$PROJECT_ROOT/ai-models"

echo "ğŸ¤– LUXOR AI CUA - æ¨¡å‹æ¢å¤åŠ©æ‰‹"
echo "================================"

# åˆ›å»ºç›®å½•ç»“æ„
echo "ğŸ“ åˆ›å»ºç›®å½•ç»“æ„..."
mkdir -p "$MODELS_DIR"/{ollama,docker-backup}
mkdir -p "$PROJECT_ROOT"/scripts

# æ¢å¤Ollamaæ¨¡å‹
echo "ğŸ“¦ æ¢å¤Ollamaæ¨¡å‹..."
if [ -f "$BACKUP_DIR/luxor9_ollama_models.tar.gz" ]; then
    cd "$MODELS_DIR/ollama/"
    tar -xzf "$BACKUP_DIR/luxor9_ollama_models.tar.gz"
    echo "âœ… Ollamaæ¨¡å‹æ¢å¤å®Œæˆ"
    
    # ç»Ÿè®¡æ¨¡å‹æ•°é‡
    model_count=$(find . -name "*.gguf" -o -name "*.bin" -o -name "*.safetensors" 2>/dev/null | wc -l)
    echo "ğŸ“Š å‘ç° $model_count ä¸ªæ¨¡å‹æ–‡ä»¶"
else
    echo "âš ï¸ Ollamaæ¨¡å‹å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
fi

# æ¢å¤Dockeré•œåƒ
echo "ğŸ³ æ¢å¤Dockeré•œåƒ..."
if [ -f "$BACKUP_DIR/images.tar.gz" ]; then
    cd "$MODELS_DIR/docker-backup/"
    tar -xzf "$BACKUP_DIR/images.tar.gz"
    echo "âœ… Dockeré•œåƒå¤‡ä»½å·²è§£å‹"
else
    echo "âš ï¸ Dockeré•œåƒå¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
fi

# ç”Ÿæˆæ¨¡å‹é…ç½®
echo "âš™ï¸ ç”Ÿæˆé…ç½®æ–‡ä»¶..."
cat > "$MODELS_DIR/model-config.yml" << EOF
# LUXOR AI CUA æ¨¡å‹é…ç½®
models:
  chinese_nlp:
    chatglm3:
      path: "./ollama/chatglm3-6b"
      type: "conversational"
      language: "zh-CN"
    qwen:
      path: "./ollama/qwen-14b"
      type: "conversational"
      language: "zh-CN"
  
  code_generation:
    codellama:
      path: "./ollama/codellama-7b"
      type: "code"
      languages: ["python", "javascript", "java"]

ollama:
  host: "localhost"
  port: 11434
  models_path: "./ollama"
EOF

# æ›´æ–°ç¯å¢ƒé…ç½®
echo "ğŸ“ æ›´æ–°é¡¹ç›®é…ç½®..."
cat >> "$PROJECT_ROOT/.env" << EOF

# AIæ¨¡å‹é…ç½® (æ¢å¤äº $(date))
OLLAMA_MODELS_PATH=$MODELS_DIR/ollama
MODEL_CONFIG_PATH=$MODELS_DIR/model-config.yml
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
EOF

# åˆ›å»ºæ¨¡å‹ç®¡ç†è„šæœ¬
echo "ğŸ› ï¸ åˆ›å»ºç®¡ç†è„šæœ¬..."
cat > "$PROJECT_ROOT/scripts/manage-models.sh" << 'EOF'
#!/bin/bash
MODELS_DIR="./ai-models/ollama"

case "$1" in
    "list")
        echo "=== æœ¬åœ°æ¨¡å‹æ–‡ä»¶ ==="
        find "$MODELS_DIR" -name "*.gguf" -o -name "*.bin" 2>/dev/null | head -10
        ;;
    "start")
        echo "=== å¯åŠ¨OllamaæœåŠ¡ ==="
        if command -v ollama &> /dev/null; then
            export OLLAMA_MODELS="$(pwd)/$MODELS_DIR"
            ollama serve &
            echo "OllamaæœåŠ¡å·²å¯åŠ¨"
        else
            echo "è¯·å…ˆå®‰è£…Ollama: curl -fsSL https://ollama.com/install.sh | sh"
        fi
        ;;
    "test")
        echo "=== æµ‹è¯•æ¨¡å‹ ==="
        model=${2:-"chatglm3"}
        echo "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹LUXOR AIé¡¹ç›®" | ollama run "$model"
        ;;
    *)
        echo "ç”¨æ³•: $0 {list|start|test}"
        ;;
esac
EOF

chmod +x "$PROJECT_ROOT/scripts/manage-models.sh"

echo ""
echo "ğŸ‰ æ¨¡å‹æ¢å¤å®Œæˆ!"
echo ""
echo "ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:"
echo "1. æŸ¥çœ‹æ¨¡å‹: ./scripts/manage-models.sh list"
echo "2. å¯åŠ¨æœåŠ¡: ./scripts/manage-models.sh start"  
echo "3. æµ‹è¯•æ¨¡å‹: ./scripts/manage-models.sh test"
echo ""
echo "ğŸ“– è¯¦ç»†è¯´æ˜è¯·æŸ¥çœ‹: BACKUP_RESTORATION_GUIDE.md"